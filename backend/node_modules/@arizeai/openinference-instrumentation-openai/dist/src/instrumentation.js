"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __rest = (this && this.__rest) || function (s, e) {
    var t = {};
    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
        t[p] = s[p];
    if (s != null && typeof Object.getOwnPropertySymbols === "function")
        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
                t[p[i]] = s[p[i]];
        }
    return t;
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenAIInstrumentation = void 0;
exports.isPatched = isPatched;
const openinference_core_1 = require("@arizeai/openinference-core");
const openinference_semantic_conventions_1 = require("@arizeai/openinference-semantic-conventions");
const api_1 = require("@opentelemetry/api");
const core_1 = require("@opentelemetry/core");
const instrumentation_1 = require("@opentelemetry/instrumentation");
const responsesAttributes_1 = require("./responsesAttributes");
const typeUtils_1 = require("./typeUtils");
const version_1 = require("./version");
const openai_1 = require("openai");
const MODULE_NAME = "openai";
const INSTRUMENTATION_NAME = "@arizeai/openinference-instrumentation-openai";
/**
 * Flag to check if the openai module has been patched
 * Note: This is a fallback in case the module is made immutable (e.x. Deno, webpack, etc.)
 */
let _isOpenInferencePatched = false;
/**
 * function to check if instrumentation is enabled / disabled
 */
function isPatched() {
    return _isOpenInferencePatched;
}
/**
 * Resolves the execution context for the current span
 * If tracing is suppressed, the span is dropped and the current context is returned
 * @param span
 */
function getExecContext(span) {
    const activeContext = api_1.context.active();
    const suppressTracing = (0, core_1.isTracingSuppressed)(activeContext);
    const execContext = suppressTracing
        ? api_1.trace.setSpan(api_1.context.active(), span)
        : activeContext;
    // Drop the span from the context
    if (suppressTracing) {
        api_1.trace.deleteSpan(activeContext);
    }
    return execContext;
}
/**
 * Gets the appropriate LLM provider based on the OpenAI client instance
 * Follows the same logic as the Python implementation by checking the baseURL host
 * @param clientInstance The OpenAI client instance
 * @returns LLMProvider.AZURE for Azure OpenAI, LLMProvider.OPENAI for regular OpenAI
 */
function getLLMProvider(clientInstance) {
    var _a;
    try {
        // The clientInstance might be a sub-object (like Completions) that has a _client property
        // pointing to the actual OpenAI/AzureOpenAI client
        const instance = clientInstance;
        let host;
        let baseURL;
        // First try to get baseURL from the instance itself
        if (instance.baseURL) {
            baseURL = instance.baseURL;
        }
        // If not found, try the _client property (this is where Azure OpenAI stores it)
        else if ((_a = instance._client) === null || _a === void 0 ? void 0 : _a.baseURL) {
            baseURL = instance._client.baseURL;
        }
        if (typeof baseURL === "string") {
            // Extract host from URL string
            try {
                const url = new URL(baseURL);
                host = url.hostname;
            }
            catch (_b) {
                // If URL parsing fails, fallback to string matching
                host = baseURL;
            }
        }
        else if (baseURL && typeof baseURL === "object" && "host" in baseURL) {
            // Direct host property
            host = baseURL.host;
        }
        if (host && typeof host === "string") {
            // Follow the same pattern as Python implementation
            if (host.includes("api.openai.com")) {
                return openinference_semantic_conventions_1.LLMProvider.OPENAI;
            }
            else if (host.includes("openai.azure.com")) {
                return openinference_semantic_conventions_1.LLMProvider.AZURE;
            }
            else if (host.includes("api.microsoft.com")) {
                // Additional Azure endpoint pattern
                return openinference_semantic_conventions_1.LLMProvider.AZURE;
            }
        }
    }
    catch (error) {
        // If we can't determine, default to regular OpenAI
        api_1.diag.debug("Failed to determine LLM provider from instance", error);
    }
    // Default to OpenAI if we can't determine
    return openinference_semantic_conventions_1.LLMProvider.OPENAI;
}
/**
 * An auto instrumentation class for OpenAI that creates {@link https://github.com/Arize-ai/openinference/blob/main/spec/semantic_conventions.md|OpenInference} Compliant spans for the OpenAI API
 * @param instrumentationConfig The config for the instrumentation @see {@link InstrumentationConfig}
 * @param traceConfig The OpenInference trace configuration. Can be used to mask or redact sensitive information on spans. @see {@link TraceConfigOptions}
 */
class OpenAIInstrumentation extends instrumentation_1.InstrumentationBase {
    constructor({ instrumentationConfig, traceConfig, tracerProvider, } = {}) {
        var _a, _b;
        super(INSTRUMENTATION_NAME, version_1.VERSION, Object.assign({}, instrumentationConfig));
        this.tracerProvider = tracerProvider;
        this.traceConfig = traceConfig;
        this.oiTracer = new openinference_core_1.OITracer({
            tracer: (_b = (_a = this.tracerProvider) === null || _a === void 0 ? void 0 : _a.getTracer(INSTRUMENTATION_NAME, version_1.VERSION)) !== null && _b !== void 0 ? _b : this.tracer,
            traceConfig,
        });
    }
    init() {
        const module = new instrumentation_1.InstrumentationNodeModuleDefinition("openai", 
        // 5.x is best effort
        ["^6.0.0", "^5.0.0"], this.patch.bind(this), this.unpatch.bind(this));
        return module;
    }
    /**
     * Manually instruments the OpenAI module. This is needed when the module is not loaded via require (commonjs)
     * @param {openai} module
     */
    manuallyInstrument(module) {
        api_1.diag.debug(`Manually instrumenting ${MODULE_NAME}`);
        this.patch(module);
    }
    get tracer() {
        if (this.tracerProvider) {
            return this.tracerProvider.getTracer(this.instrumentationName, this.instrumentationVersion);
        }
        return super.tracer;
    }
    setTracerProvider(tracerProvider) {
        super.setTracerProvider(tracerProvider);
        this.tracerProvider = tracerProvider;
        this.oiTracer = new openinference_core_1.OITracer({
            tracer: this.tracer,
            traceConfig: this.traceConfig,
        });
    }
    /**
     * Patches the OpenAI module
     */
    patch(module, moduleVersion) {
        api_1.diag.debug(`Applying patch for ${MODULE_NAME}@${moduleVersion}`);
        if ((module === null || module === void 0 ? void 0 : module.openInferencePatched) || _isOpenInferencePatched) {
            return module;
        }
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const instrumentation = this;
        this._wrap(module.OpenAI.Chat.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { messages: _messages } = body, invocationParameters = __rest(body, ["messages"]);
                const span = instrumentation.oiTracer.startSpan(`OpenAI Chat Completions`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: JSON.stringify(body), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters), [openinference_semantic_conventions_1.SemanticConventions.LLM_SYSTEM]: openinference_semantic_conventions_1.LLMSystem.OPENAI, [openinference_semantic_conventions_1.SemanticConventions.LLM_PROVIDER]: getLLMProvider(this) }, getLLMInputMessagesAttributes(body)), getLLMToolsJSONSchema(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(api_1.trace.setSpan(execContext, span), () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromiseThen = (result) => {
                    if (isChatCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, getChatCompletionLLMOutputMessagesAttributes(result)), getUsageAttributes(result)));
                        span.setStatus({ code: api_1.SpanStatusCode.OK });
                        span.end();
                    }
                    else {
                        // This is a streaming response
                        // handle the chunks and add them to the span
                        // First split the stream via tee
                        const [leftStream, rightStream] = result.tee();
                        consumeChatCompletionStreamChunks(rightStream, span);
                        result = leftStream;
                    }
                    return result;
                };
                const wrappedPromise = invokeMaybeAPIPromise(execPromise, wrappedPromiseThen);
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Completions.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { prompt: _prompt } = body, invocationParameters = __rest(body, ["prompt"]);
                const span = instrumentation.oiTracer.startSpan(`OpenAI Completions`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters), [openinference_semantic_conventions_1.SemanticConventions.LLM_SYSTEM]: openinference_semantic_conventions_1.LLMSystem.OPENAI, [openinference_semantic_conventions_1.SemanticConventions.LLM_PROVIDER]: getLLMProvider(this) }, getCompletionInputValueAndMimeType(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(api_1.trace.setSpan(execContext, span), () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromiseThen = (result) => {
                    if (isCompletionResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, getCompletionOutputValueAndMimeType(result)), getUsageAttributes(result)));
                        span.setStatus({ code: api_1.SpanStatusCode.OK });
                        span.end();
                    }
                    return result;
                };
                const wrappedPromise = invokeMaybeAPIPromise(execPromise, wrappedPromiseThen);
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        this._wrap(module.OpenAI.Embeddings.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedEmbeddingCreate(...args) {
                const body = args[0];
                const { input } = body;
                const isStringInput = typeof input === "string";
                const span = instrumentation.oiTracer.startSpan(`OpenAI Embeddings`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.EMBEDDING, [openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: isStringInput
                            ? input
                            : JSON.stringify(input), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: isStringInput
                            ? openinference_semantic_conventions_1.MimeType.TEXT
                            : openinference_semantic_conventions_1.MimeType.JSON, [openinference_semantic_conventions_1.SemanticConventions.LLM_PROVIDER]: getLLMProvider(this) }, getEmbeddingTextAttributes(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(api_1.trace.setSpan(execContext, span), () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromiseThen = (result) => {
                    if (result) {
                        // Record the results
                        span.setAttributes(Object.assign({}, getEmbeddingEmbeddingsAttributes(result)));
                    }
                    span.setStatus({ code: api_1.SpanStatusCode.OK });
                    span.end();
                    return result;
                };
                const wrappedPromise = invokeMaybeAPIPromise(execPromise, wrappedPromiseThen);
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        // Patch responses (if the patched module contains the Responses interface)
        if (module.OpenAI.Responses) {
            this._wrap(module.OpenAI.Responses.prototype, "create", 
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            (original) => {
                return function patchedCreate(...args) {
                    const body = args[0];
                    const { input: _messages } = body, invocationParameters = __rest(body, ["input"]);
                    const span = instrumentation.oiTracer.startSpan(`OpenAI Responses`, {
                        kind: api_1.SpanKind.INTERNAL,
                        attributes: Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: JSON.stringify(body), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters), [openinference_semantic_conventions_1.SemanticConventions.LLM_SYSTEM]: openinference_semantic_conventions_1.LLMSystem.OPENAI, [openinference_semantic_conventions_1.SemanticConventions.LLM_PROVIDER]: getLLMProvider(this) }, (0, responsesAttributes_1.getResponsesInputMessagesAttributes)(body)), getLLMToolsJSONSchema(body)),
                    });
                    const execContext = getExecContext(span);
                    const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                        return api_1.context.with(api_1.trace.setSpan(execContext, span), () => {
                            return original.apply(this, args);
                        });
                    }, (error) => {
                        // Push the error to the span
                        if (error) {
                            span.recordException(error);
                            span.setStatus({
                                code: api_1.SpanStatusCode.ERROR,
                                message: error.message,
                            });
                            span.end();
                        }
                    });
                    const wrappedPromiseThen = (result) => {
                        const recordSpan = (result) => {
                            if (!result) {
                                span.setStatus({ code: api_1.SpanStatusCode.ERROR });
                                span.end();
                                return;
                            }
                            span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                                // Override the model from the value sent by the server
                                [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, (0, responsesAttributes_1.getResponsesOutputMessagesAttributes)(result)), (0, responsesAttributes_1.getResponsesUsageAttributes)(result)));
                            span.setStatus({ code: api_1.SpanStatusCode.OK });
                            span.end();
                        };
                        if (isResponseCreateResponse(result)) {
                            // Record the results, as we have the final result
                            recordSpan(result);
                        }
                        else {
                            // This is a streaming response
                            // First split the stream via tee
                            const [leftStream, rightStream] = result.tee();
                            // take the right stream, consuming it and then recording the final chunk
                            // into the span
                            (0, responsesAttributes_1.consumeResponseStreamEvents)(rightStream).then(recordSpan);
                            // give the left stream back to the caller
                            result = leftStream;
                        }
                        return result;
                    };
                    const wrappedPromise = invokeMaybeAPIPromise(execPromise, wrappedPromiseThen);
                    return api_1.context.bind(execContext, wrappedPromise);
                };
            });
        }
        _isOpenInferencePatched = true;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            module.openInferencePatched = true;
        }
        catch (e) {
            api_1.diag.debug(`Failed to set ${MODULE_NAME} patched flag on the module`, e);
        }
        return module;
    }
    /**
     * Un-patches the OpenAI module's chat completions API
     */
    unpatch(moduleExports, moduleVersion) {
        api_1.diag.debug(`Removing patch for ${MODULE_NAME}@${moduleVersion}`);
        this._unwrap(moduleExports.OpenAI.Chat.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Completions.prototype, "create");
        this._unwrap(moduleExports.OpenAI.Embeddings.prototype, "create");
        _isOpenInferencePatched = false;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            moduleExports.openInferencePatched = false;
        }
        catch (e) {
            api_1.diag.warn(`Failed to unset ${MODULE_NAME} patched flag on the module`, e);
        }
    }
}
exports.OpenAIInstrumentation = OpenAIInstrumentation;
function isResponseCreateResponse(response) {
    return "object" in response && response.object === "response";
}
/**
 * type-guard that checks if the response is a chat completion response
 */
function isChatCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if the response is a completion response
 */
function isCompletionResponse(response) {
    return "choices" in response;
}
/**
 * type-guard that checks if completion prompt attribute is an array of strings
 */
function isPromptStringArray(prompt) {
    return (Array.isArray(prompt) && prompt.every((item) => typeof item === "string"));
}
/**
 * Converts the body of a chat completions request to LLM input messages
 */
function getLLMInputMessagesAttributes(body) {
    return body.messages.reduce((acc, message, index) => {
        const messageAttributes = getChatCompletionInputMessageAttributes(message);
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_INPUT_MESSAGES}.${index}.`;
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
/**
 * Converts each tool definition into a json schema
 */
function getLLMToolsJSONSchema(body) {
    if (!body.tools) {
        // If tools is undefined, return an empty object
        return {};
    }
    return body.tools.reduce((acc, tool, index) => {
        const toolJsonSchema = (0, openinference_core_1.safelyJSONStringify)(tool);
        const key = `${openinference_semantic_conventions_1.SemanticConventions.LLM_TOOLS}.${index}.${openinference_semantic_conventions_1.SemanticConventions.TOOL_JSON_SCHEMA}`;
        if (toolJsonSchema) {
            acc[key] = toolJsonSchema;
        }
        return acc;
    }, {});
}
function getChatCompletionInputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE]: role,
    };
    // Add the content only if it is a string
    if (typeof message.content === "string") {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = message.content;
    }
    else if (Array.isArray(message.content)) {
        message.content.forEach((part, index) => {
            const contentsIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENTS}.${index}.`;
            if (part.type === "text") {
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TYPE}`] = "text";
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TEXT}`] = part.text;
            }
            else if (part.type === "image_url") {
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TYPE}`] = "image";
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_IMAGE}.${openinference_semantic_conventions_1.SemanticConventions.IMAGE_URL}`] = part.image_url.url;
            }
        });
    }
    switch (role) {
        case "user":
            // There's nothing to add for the user
            break;
        case "assistant":
            if (message.tool_calls) {
                message.tool_calls.forEach((toolCall, index) => {
                    const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
                    // Add the tool call id if it exists
                    if (toolCall.id) {
                        attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] = toolCall.id;
                    }
                    // Make sure the tool call has a function
                    if (toolCall.type === "function") {
                        attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME}`] = toolCall.function.name;
                        attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON}`] = toolCall.function.arguments;
                    }
                    else {
                        // TODO: add exhaustive checks
                        api_1.diag.warn(`Unsupported tool type: ${toolCall.type}`);
                    }
                });
            }
            break;
        case "function":
            attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] = message.name;
            break;
        case "tool":
            if (message.tool_call_id) {
                attributes[`${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALL_ID}`] =
                    message.tool_call_id;
            }
            break;
        case "system":
            // There's nothing to add for the system. Content is captured above
            break;
        case "developer":
            // There's nothing to add for the developer. Content is captured above
            break;
        default:
            (0, typeUtils_1.assertUnreachable)(role);
            break;
    }
    return attributes;
}
/**
 * Converts the body of a completions request to input attributes
 */
function getCompletionInputValueAndMimeType(body) {
    if (typeof body.prompt === "string") {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: body.prompt,
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
        };
    }
    else if (isPromptStringArray(body.prompt)) {
        const prompt = body.prompt[0]; // Only single prompts are currently supported
        if (prompt === undefined) {
            return {};
        }
        return {
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: prompt,
            [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
        };
    }
    // Other cases in which the prompt is a token or array of tokens are currently unsupported
    return {};
}
/**
 * Get usage attributes
 */
function getUsageAttributes(completion) {
    var _a, _b, _c, _d;
    if (completion.usage) {
        const usageAttributes = {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: completion.usage.completion_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: completion.usage.prompt_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: completion.usage.total_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT_DETAILS_CACHE_READ]: (_a = completion.usage.prompt_tokens_details) === null || _a === void 0 ? void 0 : _a.cached_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT_DETAILS_AUDIO]: (_b = completion.usage.prompt_tokens_details) === null || _b === void 0 ? void 0 : _b.audio_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION_DETAILS_AUDIO]: (_c = completion.usage.completion_tokens_details) === null || _c === void 0 ? void 0 : _c.audio_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION_DETAILS_REASONING]: (_d = completion.usage.completion_tokens_details) === null || _d === void 0 ? void 0 : _d.reasoning_tokens,
        };
        return usageAttributes;
    }
    return {};
}
/**
 * Converts the chat completion result to LLM output attributes
 */
function getChatCompletionLLMOutputMessagesAttributes(chatCompletion) {
    // Right now support just the first choice
    const choice = chatCompletion.choices[0];
    if (!choice) {
        return {};
    }
    return [choice.message].reduce((acc, message, index) => {
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.${index}.`;
        const messageAttributes = getChatCompletionOutputMessageAttributes(message);
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
function getChatCompletionOutputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE]: role,
    };
    if (message.content) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = message.content;
    }
    if (message.tool_calls) {
        message.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Add the tool call id if it exists
            if (toolCall.id) {
                attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] = toolCall.id;
            }
            if (toolCall.type === "function") {
                attributes[toolCallIndexPrefix + openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
            else {
                // TODO: switch to exhaustive checks
                api_1.diag.warn(`Unsupported tool type: ${toolCall.type}`);
            }
        });
    }
    if (message.function_call) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            message.function_call.name;
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            message.function_call.arguments;
    }
    return attributes;
}
/**
 * Converts the completion result to output attributes
 */
function getCompletionOutputValueAndMimeType(completion) {
    // Right now support just the first choice
    const choice = completion.choices[0];
    if (!choice) {
        return {};
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: String(choice.text),
        [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
    };
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingTextAttributes(request) {
    if (typeof request.input === "string") {
        return {
            [`${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.0.${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_TEXT}`]: request.input,
        };
    }
    else if (Array.isArray(request.input) &&
        request.input.length > 0 &&
        typeof request.input[0] === "string") {
        return request.input.reduce((acc, input, index) => {
            const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
            acc[`${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_TEXT}`] = input;
            return acc;
        }, {});
    }
    // Ignore other cases where input is a number or an array of numbers
    return {};
}
/**
 * Converts the embedding result payload to embedding attributes
 */
function getEmbeddingEmbeddingsAttributes(response) {
    return response.data.reduce((acc, embedding, index) => {
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_EMBEDDINGS}.${index}.`;
        acc[`${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.EMBEDDING_VECTOR}`] =
            embedding.embedding;
        return acc;
    }, {});
}
/**
 * Consumes the stream chunks and adds them to the span
 */
function consumeChatCompletionStreamChunks(stream, span) {
    return __awaiter(this, void 0, void 0, function* () {
        var _a, stream_1, stream_1_1;
        var _b, e_1, _c, _d;
        let streamResponse = "";
        // Tool and function call attributes can also arrive in the stream
        // NB: the tools and function calls arrive in partial diffs
        // So the final tool and function calls need to be aggregated
        // across chunks
        const toolAndFunctionCallAttributes = {};
        try {
            // The first message is for the assistant response so we start at 1
            for (_a = true, stream_1 = __asyncValues(stream); stream_1_1 = yield stream_1.next(), _b = stream_1_1.done, !_b; _a = true) {
                _d = stream_1_1.value;
                _a = false;
                const chunk = _d;
                if (chunk.choices.length <= 0) {
                    continue;
                }
                const choice = chunk.choices[0];
                if (choice.delta.content) {
                    streamResponse += choice.delta.content;
                }
                // Accumulate the tool and function call attributes
                const toolAndFunctionCallAttributesDiff = getToolAndFunctionCallAttributesFromStreamChunk(chunk);
                for (const [key, value] of Object.entries(toolAndFunctionCallAttributesDiff)) {
                    if ((0, typeUtils_1.isString)(toolAndFunctionCallAttributes[key]) && (0, typeUtils_1.isString)(value)) {
                        toolAndFunctionCallAttributes[key] += value;
                    }
                    else if ((0, typeUtils_1.isString)(value)) {
                        toolAndFunctionCallAttributes[key] = value;
                    }
                }
            }
        }
        catch (e_1_1) { e_1 = { error: e_1_1 }; }
        finally {
            try {
                if (!_a && !_b && (_c = stream_1.return)) yield _c.call(stream_1);
            }
            finally { if (e_1) throw e_1.error; }
        }
        const messageIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.0.`;
        // Append the attributes to the span as a message
        const attributes = {
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: streamResponse,
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT}`]: streamResponse,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE}`]: "assistant",
        };
        // Add the tool and function call attributes
        for (const [key, value] of Object.entries(toolAndFunctionCallAttributes)) {
            attributes[`${messageIndexPrefix}${key}`] = value;
        }
        span.setAttributes(attributes);
        span.end();
    });
}
/**
 * Extracts the semantic attributes from the stream chunk for tool_calls and function_calls
 */
function getToolAndFunctionCallAttributesFromStreamChunk(chunk) {
    if (chunk.choices.length <= 0) {
        return {};
    }
    const choice = chunk.choices[0];
    const attributes = {};
    if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall, index) => {
            const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
            // Add the tool call id if it exists
            if (toolCall.id) {
                attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] = toolCall.id;
            }
            // Double check that the tool call has a function
            // NB: OpenAI only supports tool calls with functions right now but this may change
            if (toolCall.function) {
                attributes[toolCallIndexPrefix + openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME] = toolCall.function.name;
                attributes[toolCallIndexPrefix +
                    openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON] = toolCall.function.arguments;
            }
        });
    }
    if (choice.delta.function_call) {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME] =
            choice.delta.function_call.name;
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON] =
            choice.delta.function_call.arguments;
    }
    return attributes;
}
/**
 * Type-guard that checks if the promise is an APIPromise.
 *
 * APIPromise is a class from openai that wraps promises with special behavior.
 *
 * @param promise - The promise to check
 * @returns True if the promise is an APIPromise, false otherwise
 */
function isAPIPromise(promise) {
    return promise instanceof openai_1.APIPromise;
}
/**
 * Invokes the thennable of a promise or an APIPromise.
 *
 * This is necessary to safely invoke promises returned by openai sdk methods.
 *
 * Without this wrapper, instrumentation will "consume" returned APIPromise instances,
 * Making them unusable by other openai sdk methods, such as when completions.parse internally
 * calls completions.create, expecting an APIPromise which we would otherwise consume by calling `then` on it.
 *
 * @param promise - The promise to invoke the thennable of
 * @param then - The thennable to invoke
 * @returns The promise with the thennable invoked
 */
function invokeMaybeAPIPromise(promise, 
// eslint-disable-next-line @typescript-eslint/no-explicit-any
then) {
    if (isAPIPromise(promise)) {
        return promise._thenUnwrap(then);
    }
    else if (promise instanceof Promise) {
        return promise.then(then);
    }
    else {
        // eslint-disable-next-line no-console
        console.warn("Promise is not an APIPromise or a regular promise, cannot instrument.");
        return promise;
    }
}
//# sourceMappingURL=instrumentation.js.map