"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __rest = (this && this.__rest) || function (s, e) {
    var t = {};
    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
        t[p] = s[p];
    if (s != null && typeof Object.getOwnPropertySymbols === "function")
        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
                t[p[i]] = s[p[i]];
        }
    return t;
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.AnthropicInstrumentation = void 0;
exports.isPatched = isPatched;
const openinference_core_1 = require("@arizeai/openinference-core");
const openinference_semantic_conventions_1 = require("@arizeai/openinference-semantic-conventions");
const api_1 = require("@opentelemetry/api");
const core_1 = require("@opentelemetry/core");
const instrumentation_1 = require("@opentelemetry/instrumentation");
// eslint-disable-next-line @typescript-eslint/ban-ts-comment
// @ts-ignore - No version file until build
const version_1 = require("./version");
const MODULE_NAME = "@anthropic-ai/sdk";
const INSTRUMENTATION_NAME = "@arizeai/openinference-instrumentation-anthropic";
/**
 * Flag to check if the anthropic module has been patched
 * Note: This is a fallback in case the module is made immutable (e.x. Deno, webpack, etc.)
 */
let _isOpenInferencePatched = false;
/**
 * function to check if instrumentation is enabled / disabled
 */
function isPatched() {
    return _isOpenInferencePatched;
}
/**
 * Resolves the execution context for the current span
 * If tracing is suppressed, the span is dropped and the current context is returned
 * @param span
 */
function getExecContext(span) {
    const activeContext = api_1.context.active();
    const suppressTracing = (0, core_1.isTracingSuppressed)(activeContext);
    const execContext = suppressTracing
        ? api_1.trace.setSpan(api_1.context.active(), span)
        : activeContext;
    // Drop the span from the context
    if (suppressTracing) {
        api_1.trace.deleteSpan(activeContext);
    }
    return execContext;
}
/**
 * An auto instrumentation class for Anthropic that creates {@link https://github.com/Arize-ai/openinference/blob/main/spec/semantic_conventions.md|OpenInference} Compliant spans for the Anthropic API
 * @param instrumentationConfig The config for the instrumentation @see {@link InstrumentationConfig}
 * @param traceConfig The OpenInference trace configuration. Can be used to mask or redact sensitive information on spans. @see {@link TraceConfigOptions}
 */
class AnthropicInstrumentation extends instrumentation_1.InstrumentationBase {
    constructor({ instrumentationConfig, traceConfig, tracerProvider, } = {}) {
        var _a, _b;
        super(INSTRUMENTATION_NAME, version_1.VERSION, Object.assign({}, instrumentationConfig));
        this.tracerProvider = tracerProvider;
        this.traceConfig = traceConfig;
        this.oiTracer = new openinference_core_1.OITracer({
            tracer: (_b = (_a = this.tracerProvider) === null || _a === void 0 ? void 0 : _a.getTracer(INSTRUMENTATION_NAME, version_1.VERSION)) !== null && _b !== void 0 ? _b : this.tracer,
            traceConfig,
        });
    }
    init() {
        const module = new instrumentation_1.InstrumentationNodeModuleDefinition("@anthropic-ai/sdk", ["*"], // Try accepting any version
        this.patch.bind(this), this.unpatch.bind(this));
        return module;
    }
    /**
     * Manually instruments the Anthropic module. This is needed when the module is not loaded via require (commonjs)
     * @param {Anthropic} module
     */
    manuallyInstrument(module) {
        api_1.diag.debug(`Manually instrumenting ${MODULE_NAME}`);
        this.patch(module);
    }
    get tracer() {
        if (this.tracerProvider) {
            return this.tracerProvider.getTracer(this.instrumentationName, this.instrumentationVersion);
        }
        return super.tracer;
    }
    setTracerProvider(tracerProvider) {
        super.setTracerProvider(tracerProvider);
        this.tracerProvider = tracerProvider;
        this.oiTracer = new openinference_core_1.OITracer({
            tracer: this.tracer,
            traceConfig: this.traceConfig,
        });
    }
    /**
     * Patches the Anthropic module
     */
    patch(module, moduleVersion) {
        var _a, _b;
        api_1.diag.debug(`Applying patch for ${MODULE_NAME}@${moduleVersion}`);
        if ((module === null || module === void 0 ? void 0 : module.openInferencePatched) || _isOpenInferencePatched) {
            return module;
        }
        // Handle ES module default export structure
        const anthropicModule = module.default ||
            module;
        if (!((_b = (_a = anthropicModule === null || anthropicModule === void 0 ? void 0 : anthropicModule.Messages) === null || _a === void 0 ? void 0 : _a.prototype) === null || _b === void 0 ? void 0 : _b.create)) {
            api_1.diag.warn(`Cannot find Messages.prototype.create in ${MODULE_NAME}@${moduleVersion}`);
            return module;
        }
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const instrumentation = this;
        this._wrap(anthropicModule.Messages.prototype, "create", 
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (original) => {
            return function patchedCreate(...args) {
                const body = args[0];
                const { messages: _messages } = body, invocationParameters = __rest(body, ["messages"]);
                const span = instrumentation.oiTracer.startSpan(`Anthropic Messages`, {
                    kind: api_1.SpanKind.INTERNAL,
                    attributes: Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OPENINFERENCE_SPAN_KIND]: openinference_semantic_conventions_1.OpenInferenceSpanKind.LLM, [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: body.model, [openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE]: JSON.stringify(body), [openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, [openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS]: JSON.stringify(invocationParameters), [openinference_semantic_conventions_1.SemanticConventions.LLM_SYSTEM]: openinference_semantic_conventions_1.LLMSystem.ANTHROPIC, [openinference_semantic_conventions_1.SemanticConventions.LLM_PROVIDER]: openinference_semantic_conventions_1.LLMProvider.ANTHROPIC }, getAnthropicInputMessagesAttributes(body)), getAnthropicToolsJSONSchema(body)),
                });
                const execContext = getExecContext(span);
                const execPromise = (0, instrumentation_1.safeExecuteInTheMiddle)(() => {
                    return api_1.context.with(api_1.trace.setSpan(execContext, span), () => {
                        return original.apply(this, args);
                    });
                }, (error) => {
                    // Push the error to the span
                    if (error) {
                        span.recordException(error);
                        span.setStatus({
                            code: api_1.SpanStatusCode.ERROR,
                            message: error.message,
                        });
                        span.end();
                    }
                });
                const wrappedPromiseThen = (result) => {
                    if (isAnthropicMessageResponse(result)) {
                        // Record the results
                        span.setAttributes(Object.assign(Object.assign({ [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: JSON.stringify(result), [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.JSON, 
                            // Override the model from the value sent by the server
                            [openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME]: result.model }, getAnthropicOutputMessagesAttributes(result)), getAnthropicUsageAttributes(result)));
                        span.setStatus({ code: api_1.SpanStatusCode.OK });
                        span.end();
                    }
                    else if (isAnthropicStream(result)) {
                        // This is a streaming response
                        // handle the chunks and add them to the span
                        // First split the stream via tee
                        const [leftStream, rightStream] = result.tee();
                        consumeAnthropicStreamChunks(rightStream, span);
                        result = leftStream;
                    }
                    return result;
                };
                const wrappedPromise = execPromise
                    .then(wrappedPromiseThen)
                    .catch((error) => {
                    span.recordException(error);
                    span.setStatus({
                        code: api_1.SpanStatusCode.ERROR,
                        message: error.message,
                    });
                    span.end();
                    throw error;
                });
                return api_1.context.bind(execContext, wrappedPromise);
            };
        });
        _isOpenInferencePatched = true;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            module.openInferencePatched = true;
        }
        catch (e) {
            api_1.diag.debug(`Failed to set ${MODULE_NAME} patched flag on the module`, e);
        }
        return module;
    }
    /**
     * Un-patches the Anthropic module's messages API
     */
    unpatch(moduleExports, moduleVersion) {
        api_1.diag.debug(`Removing patch for ${MODULE_NAME}@${moduleVersion}`);
        this._unwrap(moduleExports.Messages.prototype, "create");
        _isOpenInferencePatched = false;
        try {
            // This can fail if the module is made immutable via the runtime or bundler
            moduleExports.openInferencePatched = false;
        }
        catch (e) {
            api_1.diag.warn(`Failed to unset ${MODULE_NAME} patched flag on the module`, e);
        }
    }
}
exports.AnthropicInstrumentation = AnthropicInstrumentation;
/**
 * type-guard that checks if the response is an Anthropic message response
 */
function isAnthropicMessageResponse(response) {
    return (response != null &&
        typeof response === "object" &&
        "content" in response &&
        "role" in response);
}
/**
 * type-guard that checks if the response is an Anthropic stream
 */
function isAnthropicStream(response) {
    return response != null && typeof response === "object" && "tee" in response;
}
/**
 * Converts the body of an Anthropic messages request to LLM input messages
 */
function getAnthropicInputMessagesAttributes(body) {
    return body.messages.reduce((acc, message, index) => {
        const messageAttributes = getAnthropicInputMessageAttributes(message);
        const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_INPUT_MESSAGES}.${index}.`;
        // Flatten the attributes on the index prefix
        for (const [key, value] of Object.entries(messageAttributes)) {
            acc[`${indexPrefix}${key}`] = value;
        }
        return acc;
    }, {});
}
/**
 * Converts each tool definition into a json schema
 */
function getAnthropicToolsJSONSchema(body) {
    if (!body.tools) {
        // If tools is undefined, return an empty object
        return {};
    }
    return body.tools.reduce((acc, tool, index) => {
        const toolJsonSchema = (0, openinference_core_1.safelyJSONStringify)(tool);
        const key = `${openinference_semantic_conventions_1.SemanticConventions.LLM_TOOLS}.${index}.${openinference_semantic_conventions_1.SemanticConventions.TOOL_JSON_SCHEMA}`;
        if (toolJsonSchema) {
            acc[key] = toolJsonSchema;
        }
        return acc;
    }, {});
}
function getAnthropicInputMessageAttributes(message) {
    const role = message.role;
    const attributes = {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE]: role,
    };
    // Add the content based on type
    if (typeof message.content === "string") {
        attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = message.content;
    }
    else if (Array.isArray(message.content)) {
        message.content.forEach((part, index) => {
            const contentsIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENTS}.${index}.`;
            if (part.type === "text") {
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TYPE}`] = "text";
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TEXT}`] = part.text;
            }
            else if (part.type === "image") {
                attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TYPE}`] = "image";
                if (part.source.type === "base64") {
                    // For base64 images, we don't store the actual data but indicate it's base64
                    attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_IMAGE}.type`] = "base64";
                    attributes[`${contentsIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_IMAGE}.media_type`] = part.source.media_type;
                }
            }
            else if (part.type === "tool_use") {
                const toolCallIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${index}.`;
                attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] = part.id;
                attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME}`] = part.name;
                attributes[`${toolCallIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON}`] = JSON.stringify(part.input);
            }
            else if (part.type === "tool_result") {
                attributes[`${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALL_ID}`] =
                    part.tool_use_id;
                if (typeof part.content === "string") {
                    attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = part.content;
                }
                else if (Array.isArray(part.content)) {
                    // Handle complex tool result content
                    attributes[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = JSON.stringify(part.content);
                }
            }
        });
    }
    return attributes;
}
/**
 * Converts the Anthropic message result to LLM output attributes
 */
function getAnthropicOutputMessagesAttributes(message) {
    const attributes = {};
    const indexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.0.`;
    attributes[`${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE}`] =
        message.role;
    // Handle content array
    message.content.forEach((content, contentIndex) => {
        const contentPrefix = `${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENTS}.${contentIndex}.`;
        if (content.type === "text") {
            attributes[`${contentPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TYPE}`] = "text";
            attributes[`${contentPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT_TEXT}`] = content.text;
        }
        else if (content.type === "tool_use") {
            const toolCallPrefix = `${indexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${contentIndex}.`;
            attributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] =
                content.id;
            attributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME}`] = content.name;
            attributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON}`] = JSON.stringify(content.input);
        }
    });
    return attributes;
}
/**
 * Get usage attributes from Anthropic response
 */
function getAnthropicUsageAttributes(message) {
    if (message.usage) {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: message.usage.output_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: message.usage.input_tokens,
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: message.usage.input_tokens + message.usage.output_tokens,
        };
    }
    return {};
}
/**
 * Consumes the stream chunks and adds them to the span
 */
function consumeAnthropicStreamChunks(stream, span) {
    return __awaiter(this, void 0, void 0, function* () {
        var _a, stream_1, stream_1_1;
        var _b, e_1, _c, _d;
        let streamResponse = "";
        const toolCallAttributes = {};
        try {
            for (_a = true, stream_1 = __asyncValues(stream); stream_1_1 = yield stream_1.next(), _b = stream_1_1.done, !_b; _a = true) {
                _d = stream_1_1.value;
                _a = false;
                const chunk = _d;
                if (chunk.type === "content_block_delta" &&
                    chunk.delta.type === "text_delta") {
                    streamResponse += chunk.delta.text;
                }
                else if (chunk.type === "content_block_start" &&
                    chunk.content_block.type === "tool_use") {
                    const toolCall = chunk.content_block;
                    const toolCallIndex = chunk.index;
                    const toolCallPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${toolCallIndex}.`;
                    toolCallAttributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_ID}`] = toolCall.id;
                    toolCallAttributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME}`] = toolCall.name;
                }
                else if (chunk.type === "content_block_delta" &&
                    chunk.delta.type === "input_json_delta") {
                    const toolCallIndex = chunk.index;
                    const toolCallPrefix = `${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS}.${toolCallIndex}.`;
                    const existingArgs = toolCallAttributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON}`] || "";
                    toolCallAttributes[`${toolCallPrefix}${openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON}`] = existingArgs + chunk.delta.partial_json;
                }
            }
        }
        catch (e_1_1) { e_1 = { error: e_1_1 }; }
        finally {
            try {
                if (!_a && !_b && (_c = stream_1.return)) yield _c.call(stream_1);
            }
            finally { if (e_1) throw e_1.error; }
        }
        const messageIndexPrefix = `${openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES}.0.`;
        // Append the attributes to the span as a message
        const attributes = {
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE]: streamResponse,
            [openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE]: openinference_semantic_conventions_1.MimeType.TEXT,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT}`]: streamResponse,
            [`${messageIndexPrefix}${openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE}`]: "assistant",
        };
        // Add the tool call attributes
        for (const [key, value] of Object.entries(toolCallAttributes)) {
            attributes[`${messageIndexPrefix}${key}`] = value;
        }
        span.setAttributes(attributes);
        span.setStatus({ code: api_1.SpanStatusCode.OK });
        span.end();
    });
}
//# sourceMappingURL=instrumentation.js.map