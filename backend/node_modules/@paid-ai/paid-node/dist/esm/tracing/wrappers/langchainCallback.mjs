var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
import { BaseCallbackHandler } from "@langchain/core/callbacks/base";
import { SpanStatusCode } from "@opentelemetry/api";
import { getPaidTracer, getToken } from "tracing/tracing.js";
import { getTracingContext } from "tracing/tracingContext.js";
export class PaidLangChainCallback extends BaseCallbackHandler {
    constructor() {
        super();
        this.name = "PaidLangChainCallback";
        this.spans = new Map();
        const tracer = getPaidTracer();
        if (!tracer) {
            throw new Error("Paid tracer is not initialized, Make sure to call 'initializeTracing()' first");
        }
        this.tracer = tracer;
    }
    extractProvider(serialized) {
        if (!(serialized === null || serialized === void 0 ? void 0 : serialized.id))
            return "unknown";
        const idStr = serialized.id.join(" ").toLowerCase();
        if (idStr.includes("openai"))
            return "openai";
        if (idStr.includes("anthropic"))
            return "anthropic";
        if (idStr.includes("mistral"))
            return "mistral";
        if (idStr.includes("cohere"))
            return "cohere";
        if (idStr.includes("huggingface"))
            return "huggingface";
        if (idStr.includes("azure"))
            return "azure";
        return "unknown";
    }
    handleLLMStart(llm, prompts, runId, parentRunId, extraParams, tags, metadata) {
        return __awaiter(this, void 0, void 0, function* () {
            const { externalProductId, externalCustomerId } = getTracingContext();
            const token = getToken();
            if (!token || !externalCustomerId) {
                throw new Error("No token or externalCustomerId: This wrapper should be used inside a callback to paid.trace().");
            }
            const modelType = (metadata === null || metadata === void 0 ? void 0 : metadata.ls_model_type) || "unknown";
            const modelName = (metadata === null || metadata === void 0 ? void 0 : metadata.ls_model_name) || "unknown";
            const spanName = `trace.langchain.${modelType}`;
            const span = this.tracer.startSpan(spanName);
            const attributes = {
                "gen_ai.system": this.extractProvider(llm),
                "gen_ai.operation.name": modelType,
                "gen_ai.request.model": modelName,
                external_customer_id: externalCustomerId,
                token: token,
            };
            if (externalProductId) {
                attributes["external_agent_id"] = externalProductId;
            }
            span.setAttributes(attributes);
            this.spans.set(runId, span);
        });
    }
    handleLLMEnd(output, runId, parentRunId) {
        return __awaiter(this, void 0, void 0, function* () {
            var _a, _b, _c, _d, _e, _f, _g, _h, _j;
            const span = this.spans.get(runId);
            if (!span)
                return;
            try {
                const attributes = {};
                // Extract token usage information
                if ((_a = output.llmOutput) === null || _a === void 0 ? void 0 : _a.tokenUsage) {
                    const usage = output.llmOutput.tokenUsage;
                    if (usage.promptTokens !== undefined) {
                        attributes["gen_ai.usage.input_tokens"] = usage.promptTokens;
                    }
                    if (usage.completionTokens !== undefined) {
                        attributes["gen_ai.usage.output_tokens"] = usage.completionTokens;
                    }
                    if (usage.cachedInputTokens !== undefined) {
                        attributes["gen_ai.usage.cached_input_tokens"] = usage.cachedInputTokens;
                    }
                    if (usage.reasoningOutputTokens !== undefined) {
                        attributes["gen_ai.usage.reasoning_output_tokens"] = usage.reasoningOutputTokens;
                    }
                }
                // Alternative token usage format (some providers use different field names)
                if ((_b = output.llmOutput) === null || _b === void 0 ? void 0 : _b.token_usage) {
                    const usage = output.llmOutput.token_usage;
                    if (usage.prompt_tokens !== undefined) {
                        attributes["gen_ai.usage.input_tokens"] = usage.prompt_tokens;
                    }
                    if (usage.completion_tokens !== undefined) {
                        attributes["gen_ai.usage.output_tokens"] = usage.completion_tokens;
                    }
                    if (usage.cached_input_tokens !== undefined) {
                        attributes["gen_ai.usage.cached_input_tokens"] = usage.cached_input_tokens;
                    }
                    if (usage.reasoning_output_tokens !== undefined) {
                        attributes["gen_ai.usage.reasoning_output_tokens"] = usage.reasoning_output_tokens;
                    }
                }
                // Add model from response if available
                if ((_c = output.llmOutput) === null || _c === void 0 ? void 0 : _c.modelName) {
                    attributes["gen_ai.response.model"] = output.llmOutput.modelName;
                }
                else if ((_d = output.llmOutput) === null || _d === void 0 ? void 0 : _d.model_name) {
                    attributes["gen_ai.response.model"] = output.llmOutput.model_name;
                }
                else if ((_j = (_h = (_g = (_f = (_e = output.generations) === null || _e === void 0 ? void 0 : _e[0]) === null || _f === void 0 ? void 0 : _f[0]) === null || _g === void 0 ? void 0 : _g.message) === null || _h === void 0 ? void 0 : _h["response_metadata"]) === null || _j === void 0 ? void 0 : _j.model_name) {
                    attributes["gen_ai.response.model"] = output.generations[0][0].message["response_metadata"].model_name;
                }
                span.setAttributes(attributes);
                span.setStatus({ code: SpanStatusCode.OK });
            }
            finally {
                span.end();
                this.spans.delete(runId);
            }
        });
    }
    handleLLMError(err, runId, parentRunId) {
        return __awaiter(this, void 0, void 0, function* () {
            const span = this.spans.get(runId);
            if (!span)
                return;
            try {
                span.setStatus({ code: SpanStatusCode.ERROR, message: err.message });
                span.recordException(err);
            }
            finally {
                span.end();
                this.spans.delete(runId);
            }
        });
    }
}
