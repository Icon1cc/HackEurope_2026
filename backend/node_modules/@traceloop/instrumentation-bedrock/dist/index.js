'use strict';

var tslib = require('tslib');
var api = require('@opentelemetry/api');
var instrumentation = require('@opentelemetry/instrumentation');
var aiSemanticConventions = require('@traceloop/ai-semantic-conventions');
var incubating = require('@opentelemetry/semantic-conventions/incubating');

var version = "0.22.6";

class BedrockInstrumentation extends instrumentation.InstrumentationBase {
    constructor(config = {}) {
        super("@traceloop/instrumentation-bedrock", version, config);
    }
    setConfig(config = {}) {
        super.setConfig(config);
    }
    init() {
        const module = new instrumentation.InstrumentationNodeModuleDefinition("@aws-sdk/client-bedrock-runtime", [">=3.499.0"], this.wrap.bind(this), this.unwrap.bind(this));
        return module;
    }
    manuallyInstrument(module) {
        this._diag.debug(`Patching @aws-sdk/client-bedrock-runtime manually`);
        this._wrap(module.BedrockRuntimeClient.prototype, "send", this.wrapperMethod());
    }
    wrap(module, moduleVersion) {
        this._diag.debug(`Patching @aws-sdk/client-bedrock-runtime@${moduleVersion}`);
        this._wrap(module.BedrockRuntimeClient.prototype, "send", this.wrapperMethod());
        return module;
    }
    unwrap(module, moduleVersion) {
        this._diag.debug(`Unpatching @aws-sdk/client-bedrock-runtime@${moduleVersion}`);
        this._unwrap(module.BedrockRuntimeClient.prototype, "send");
    }
    wrapperMethod() {
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const plugin = this;
        // eslint-disable-next-line
        return (original) => {
            return function method(...args) {
                const span = plugin._startSpan({
                    params: args[0],
                });
                const execContext = api.trace.setSpan(api.context.active(), span);
                const execPromise = instrumentation.safeExecuteInTheMiddle(() => {
                    return api.context.with(execContext, () => {
                        return original.apply(this, args);
                    });
                }, (e) => {
                    if (e) {
                        plugin._diag.error(`Error in bedrock instrumentation`, e);
                    }
                });
                const wrappedPromise = plugin._wrapPromise(span, execPromise);
                return api.context.bind(execContext, wrappedPromise);
            };
        };
    }
    _wrapPromise(span, promise) {
        return promise
            .then((result) => tslib.__awaiter(this, void 0, void 0, function* () {
            yield this._endSpan({
                span,
                result: result,
            });
            return new Promise((resolve) => resolve(result));
        }))
            .catch((error) => {
            return new Promise((_, reject) => {
                span.setStatus({
                    code: api.SpanStatusCode.ERROR,
                    message: error.message,
                });
                span.recordException(error);
                span.end();
                reject(error);
            });
        });
    }
    _startSpan({ params, }) {
        var _a, _b;
        let attributes = {};
        try {
            const input = params.input;
            const { modelVendor, model } = this._extractVendorAndModel(input.modelId || "");
            attributes = {
                [incubating.ATTR_GEN_AI_SYSTEM]: "AWS",
                [incubating.ATTR_GEN_AI_REQUEST_MODEL]: model,
                [incubating.ATTR_GEN_AI_RESPONSE_MODEL]: input.modelId,
                [aiSemanticConventions.SpanAttributes.LLM_REQUEST_TYPE]: aiSemanticConventions.LLMRequestTypeValues.COMPLETION,
            };
            if (typeof input.body === "string") {
                const requestBody = JSON.parse(input.body);
                attributes = Object.assign(Object.assign({}, attributes), this._setRequestAttributes(modelVendor, requestBody));
            }
        }
        catch (e) {
            this._diag.debug(e);
            (_b = (_a = this._config).exceptionLogger) === null || _b === void 0 ? void 0 : _b.call(_a, e);
        }
        return this.tracer.startSpan(`bedrock.completion`, {
            kind: api.SpanKind.CLIENT,
            attributes,
        });
    }
    _endSpan(_a) {
        return tslib.__awaiter(this, arguments, void 0, function* ({ span, result, }) {
            var _b, e_1, _c, _d;
            var _e, _f, _g;
            try {
                if ("body" in result) {
                    const attributes = "attributes" in span
                        ? span["attributes"]
                        : {};
                    if (incubating.ATTR_GEN_AI_SYSTEM in attributes) {
                        const modelId = attributes[incubating.ATTR_GEN_AI_RESPONSE_MODEL];
                        const { modelVendor, model } = this._extractVendorAndModel(modelId);
                        span.setAttribute(incubating.ATTR_GEN_AI_RESPONSE_MODEL, model);
                        if (!(result.body instanceof Object.getPrototypeOf(Uint8Array))) {
                            const rawRes = result.body;
                            let streamedContent = "";
                            try {
                                for (var _h = true, rawRes_1 = tslib.__asyncValues(rawRes), rawRes_1_1; rawRes_1_1 = yield rawRes_1.next(), _b = rawRes_1_1.done, !_b; _h = true) {
                                    _d = rawRes_1_1.value;
                                    _h = false;
                                    const value = _d;
                                    // Convert it to a JSON String
                                    const jsonString = new TextDecoder().decode((_e = value.chunk) === null || _e === void 0 ? void 0 : _e.bytes);
                                    // Parse the JSON string
                                    const parsedResponse = JSON.parse(jsonString);
                                    if ("amazon-bedrock-invocationMetrics" in parsedResponse) {
                                        span.setAttribute(incubating.ATTR_GEN_AI_USAGE_PROMPT_TOKENS, parsedResponse["amazon-bedrock-invocationMetrics"]["inputTokenCount"]);
                                        span.setAttribute(incubating.ATTR_GEN_AI_USAGE_COMPLETION_TOKENS, parsedResponse["amazon-bedrock-invocationMetrics"]["outputTokenCount"]);
                                        span.setAttribute(aiSemanticConventions.SpanAttributes.LLM_USAGE_TOTAL_TOKENS, parsedResponse["amazon-bedrock-invocationMetrics"]["inputTokenCount"] +
                                            parsedResponse["amazon-bedrock-invocationMetrics"]["outputTokenCount"]);
                                    }
                                    let responseAttributes = this._setResponseAttributes(modelVendor, parsedResponse, true);
                                    // ! NOTE: This make sure the content always have all streamed chunks
                                    if (this._shouldSendPrompts()) {
                                        // Update local value with attribute value that was set by _setResponseAttributes
                                        streamedContent +=
                                            responseAttributes[`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`];
                                        // re-assign the new value to responseAttributes
                                        responseAttributes = Object.assign(Object.assign({}, responseAttributes), { [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: streamedContent });
                                    }
                                    span.setAttributes(responseAttributes);
                                }
                            }
                            catch (e_1_1) { e_1 = { error: e_1_1 }; }
                            finally {
                                try {
                                    if (!_h && !_b && (_c = rawRes_1.return)) yield _c.call(rawRes_1);
                                }
                                finally { if (e_1) throw e_1.error; }
                            }
                        }
                        else if (result.body instanceof Object.getPrototypeOf(Uint8Array)) {
                            // Convert it to a JSON String
                            const jsonString = new TextDecoder().decode(result.body);
                            // Parse the JSON string
                            const parsedResponse = JSON.parse(jsonString);
                            const responseAttributes = this._setResponseAttributes(modelVendor, parsedResponse);
                            span.setAttributes(responseAttributes);
                        }
                    }
                }
            }
            catch (e) {
                this._diag.debug(e);
                (_g = (_f = this._config).exceptionLogger) === null || _g === void 0 ? void 0 : _g.call(_f, e);
            }
            span.setStatus({ code: api.SpanStatusCode.OK });
            span.end();
        });
    }
    _setRequestAttributes(vendor, requestBody) {
        switch (vendor) {
            case "ai21": {
                return Object.assign({ [incubating.ATTR_GEN_AI_REQUEST_TOP_P]: requestBody["topP"], [incubating.ATTR_GEN_AI_REQUEST_TEMPERATURE]: requestBody["temperature"], [incubating.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: requestBody["maxTokens"], [aiSemanticConventions.SpanAttributes.LLM_PRESENCE_PENALTY]: requestBody["presencePenalty"]["scale"], [aiSemanticConventions.SpanAttributes.LLM_FREQUENCY_PENALTY]: requestBody["frequencyPenalty"]["scale"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.role`]: "user",
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.content`]: requestBody["prompt"],
                    }
                    : {}));
            }
            case "amazon": {
                return Object.assign({ [incubating.ATTR_GEN_AI_REQUEST_TOP_P]: requestBody["textGenerationConfig"]["topP"], [incubating.ATTR_GEN_AI_REQUEST_TEMPERATURE]: requestBody["textGenerationConfig"]["temperature"], [incubating.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: requestBody["textGenerationConfig"]["maxTokenCount"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.role`]: "user",
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.content`]: requestBody["inputText"],
                    }
                    : {}));
            }
            case "anthropic": {
                const baseAttributes = {
                    [incubating.ATTR_GEN_AI_REQUEST_TOP_P]: requestBody["top_p"],
                    [aiSemanticConventions.SpanAttributes.LLM_TOP_K]: requestBody["top_k"],
                    [incubating.ATTR_GEN_AI_REQUEST_TEMPERATURE]: requestBody["temperature"],
                    [incubating.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: requestBody["max_tokens_to_sample"] || requestBody["max_tokens"],
                };
                if (!this._shouldSendPrompts()) {
                    return baseAttributes;
                }
                // Handle new messages API format (used by langchain)
                if (requestBody["messages"]) {
                    const promptAttributes = {};
                    requestBody["messages"].forEach((message, index) => {
                        promptAttributes[`${incubating.ATTR_GEN_AI_PROMPT}.${index}.role`] =
                            message.role;
                        promptAttributes[`${incubating.ATTR_GEN_AI_PROMPT}.${index}.content`] =
                            typeof message.content === "string"
                                ? message.content
                                : JSON.stringify(message.content);
                    });
                    return Object.assign(Object.assign({}, baseAttributes), promptAttributes);
                }
                // Handle legacy prompt format
                if (requestBody["prompt"]) {
                    return Object.assign(Object.assign({}, baseAttributes), { [`${incubating.ATTR_GEN_AI_PROMPT}.0.role`]: "user", [`${incubating.ATTR_GEN_AI_PROMPT}.0.content`]: requestBody["prompt"]
                            // The format is removing when we are setting span attribute
                            .replace("\n\nHuman:", "")
                            .replace("\n\nAssistant:", "") });
                }
                return baseAttributes;
            }
            case "cohere": {
                return Object.assign({ [incubating.ATTR_GEN_AI_REQUEST_TOP_P]: requestBody["p"], [aiSemanticConventions.SpanAttributes.LLM_TOP_K]: requestBody["k"], [incubating.ATTR_GEN_AI_REQUEST_TEMPERATURE]: requestBody["temperature"], [incubating.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: requestBody["max_tokens"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.role`]: "user",
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.content`]: requestBody["message"] || requestBody["prompt"],
                    }
                    : {}));
            }
            case "meta": {
                return Object.assign({ [incubating.ATTR_GEN_AI_REQUEST_TOP_P]: requestBody["top_p"], [incubating.ATTR_GEN_AI_REQUEST_TEMPERATURE]: requestBody["temperature"], [incubating.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: requestBody["max_gen_len"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.role`]: "user",
                        [`${incubating.ATTR_GEN_AI_PROMPT}.0.content`]: requestBody["prompt"],
                    }
                    : {}));
            }
            default:
                return {};
        }
    }
    _setResponseAttributes(vendor, response, isStream = false) {
        var _a, _b, _c, _d;
        switch (vendor) {
            case "ai21": {
                return Object.assign({ [`${incubating.ATTR_GEN_AI_COMPLETION}.0.finish_reason`]: response["completions"][0]["finishReason"]["reason"], [`${incubating.ATTR_GEN_AI_COMPLETION}.0.role`]: "assistant" }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: response["completions"][0]["data"]["text"],
                    }
                    : {}));
            }
            case "amazon": {
                return Object.assign({ [`${incubating.ATTR_GEN_AI_COMPLETION}.0.finish_reason`]: isStream
                        ? response["completionReason"]
                        : response["results"][0]["completionReason"], [`${incubating.ATTR_GEN_AI_COMPLETION}.0.role`]: "assistant", [incubating.ATTR_GEN_AI_USAGE_PROMPT_TOKENS]: response["inputTextTokenCount"], [incubating.ATTR_GEN_AI_USAGE_COMPLETION_TOKENS]: isStream
                        ? response["totalOutputTextTokenCount"]
                        : response["results"][0]["tokenCount"], [aiSemanticConventions.SpanAttributes.LLM_USAGE_TOTAL_TOKENS]: isStream
                        ? response["inputTextTokenCount"] +
                            response["totalOutputTextTokenCount"]
                        : response["inputTextTokenCount"] +
                            response["results"][0]["tokenCount"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: isStream
                            ? response["outputText"]
                            : response["results"][0]["outputText"],
                    }
                    : {}));
            }
            case "anthropic": {
                const baseAttributes = {
                    [`${incubating.ATTR_GEN_AI_COMPLETION}.0.finish_reason`]: response["stop_reason"],
                    [`${incubating.ATTR_GEN_AI_COMPLETION}.0.role`]: "assistant",
                };
                if (!this._shouldSendPrompts()) {
                    return baseAttributes;
                }
                // Handle new messages API format response
                if (response["content"]) {
                    const content = Array.isArray(response["content"])
                        ? response["content"].map((c) => c.text || c).join("")
                        : response["content"];
                    return Object.assign(Object.assign({}, baseAttributes), { [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: content });
                }
                // Handle legacy completion format
                if (response["completion"]) {
                    return Object.assign(Object.assign({}, baseAttributes), { [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: response["completion"] });
                }
                return baseAttributes;
            }
            case "cohere": {
                const baseAttributes = Object.assign({ [`${incubating.ATTR_GEN_AI_COMPLETION}.0.finish_reason`]: (_b = (_a = response["generations"]) === null || _a === void 0 ? void 0 : _a[0]) === null || _b === void 0 ? void 0 : _b["finish_reason"], [`${incubating.ATTR_GEN_AI_COMPLETION}.0.role`]: "assistant" }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: (_d = (_c = response["generations"]) === null || _c === void 0 ? void 0 : _c[0]) === null || _d === void 0 ? void 0 : _d["text"],
                    }
                    : {}));
                // Add token usage if available
                if (response["meta"] && response["meta"]["billed_units"]) {
                    const billedUnits = response["meta"]["billed_units"];
                    return Object.assign(Object.assign({}, baseAttributes), { [incubating.ATTR_GEN_AI_USAGE_PROMPT_TOKENS]: billedUnits["input_tokens"], [incubating.ATTR_GEN_AI_USAGE_COMPLETION_TOKENS]: billedUnits["output_tokens"], [aiSemanticConventions.SpanAttributes.LLM_USAGE_TOTAL_TOKENS]: (billedUnits["input_tokens"] || 0) +
                            (billedUnits["output_tokens"] || 0) });
                }
                return baseAttributes;
            }
            case "meta": {
                return Object.assign({ [`${incubating.ATTR_GEN_AI_COMPLETION}.0.finish_reason`]: response["stop_reason"], [`${incubating.ATTR_GEN_AI_COMPLETION}.0.role`]: "assistant", [incubating.ATTR_GEN_AI_USAGE_PROMPT_TOKENS]: response["prompt_token_count"], [incubating.ATTR_GEN_AI_USAGE_COMPLETION_TOKENS]: response["generation_token_count"], [aiSemanticConventions.SpanAttributes.LLM_USAGE_TOTAL_TOKENS]: response["prompt_token_count"] + response["generation_token_count"] }, (this._shouldSendPrompts()
                    ? {
                        [`${incubating.ATTR_GEN_AI_COMPLETION}.0.content`]: response["generation"],
                    }
                    : {}));
            }
            default:
                return {};
        }
    }
    _shouldSendPrompts() {
        const contextShouldSendPrompts = api.context
            .active()
            .getValue(aiSemanticConventions.CONTEXT_KEY_ALLOW_TRACE_CONTENT);
        if (contextShouldSendPrompts !== undefined) {
            return contextShouldSendPrompts;
        }
        return this._config.traceContent !== undefined
            ? this._config.traceContent
            : true;
    }
    _extractVendorAndModel(modelId) {
        if (!modelId) {
            return { modelVendor: "", model: "" };
        }
        const parts = modelId.split(".");
        return {
            modelVendor: parts[0] || "",
            model: parts[1] || "",
        };
    }
}

exports.BedrockInstrumentation = BedrockInstrumentation;
//# sourceMappingURL=index.js.map
