{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"PYTHONUTF8\", \"1\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# repo root → makes `processing_layer` importable\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "load_dotenv(Path().resolve().parent.parent / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 samples from HuggingFace…\n",
      "Downloading config file fiftyone.yml from Voxel51/high-quality-invoice-images-for-ocr\n",
      "Loading dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dataset name 'Voxel51/high-quality-invoice-images-for-ocr' is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_23928\\422668118.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m N_SAMPLES = \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# adjust as needed\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m print(f\"Loading {N_SAMPLES} samples from HuggingFace…\")\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = load_from_hub(\n\u001b[32m      8\u001b[39m     \u001b[33m\"Voxel51/high-quality-invoice-images-for-ocr\"\u001b[39m,\n\u001b[32m      9\u001b[39m     max_samples=N_SAMPLES,\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(repo_id, revision, split, splits, subset, subsets, max_samples, batch_size, num_workers, overwrite, persistent, name, token, config_file, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m     config = _get_dataset_metadata(repo_id, revision=revision, **kwargs)\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Could not find fiftyone metadata for {repo_id}\")\n\u001b[32m    322\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_dataset_from_config(config, **kwargs)\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(config, **kwargs)\u001b[39m\n\u001b[32m    895\u001b[39m     _ensure_dataset_compatibility(config)\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isinstance(config, HFHubParquetFilesDatasetConfig):\n\u001b[32m    897\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load_parquet_files_dataset_from_config(config, **kwargs)\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load_fiftyone_dataset_from_config(config, **kwargs)\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\utils\\huggingface.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(config, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m     name = _resolve_dataset_name(config, **kwargs)\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1498\u001b[39m         dataset_kwargs[\u001b[33m\"name\"\u001b[39m] = name\n\u001b[32m   1499\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m     dataset = fod.Dataset.from_dir(download_dir, **dataset_kwargs)\n\u001b[32m   1501\u001b[39m \n\u001b[32m   1502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dataset_type_name != \u001b[33m\"FiftyOneDataset\"\u001b[39m:\n\u001b[32m   1503\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, dataset_dir, dataset_type, data_path, labels_path, name, persistent, overwrite, label_field, tags, dynamic, progress, **kwargs)\u001b[39m\n\u001b[32m   7855\u001b[39m \n\u001b[32m   7856\u001b[39m         Returns:\n\u001b[32m   7857\u001b[39m             a :\u001b[38;5;28;01mclass\u001b[39;00m:`Dataset`\n\u001b[32m   7858\u001b[39m         \"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m7859\u001b[39m         dataset = cls(name, persistent=persistent, overwrite=overwrite)\n\u001b[32m   7860\u001b[39m         dataset.add_dir(\n\u001b[32m   7861\u001b[39m             dataset_dir=dataset_dir,\n\u001b[32m   7862\u001b[39m             dataset_type=dataset_type,\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\singletons.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, name, _create, _reload, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m instance \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     48\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m instance.deleted\n\u001b[32m     49\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m instance.name \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     50\u001b[39m         ):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m             instance = cls._make_instance(\n\u001b[32m     52\u001b[39m                 name=name, _create=_create, *args, **kwargs\n\u001b[32m     53\u001b[39m             )\n\u001b[32m     54\u001b[39m             name = instance.name  \u001b[38;5;66;03m# `__init__` may have changed `name`\u001b[39;00m\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\singletons.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, name, _create, *args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _make_instance(cls, name, _create, *args, **kwargs):\n\u001b[32m     34\u001b[39m         instance = cls.__new__(cls)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         instance.__init__(name=name, _create=_create, *args, **kwargs)\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, persistent, overwrite, _create, _virtual, _reload, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;28;01mand\u001b[39;00m dataset_exists(name):\n\u001b[32m    320\u001b[39m             delete_dataset(name)\n\u001b[32m    321\u001b[39m \n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _create:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m             doc, sample_doc_cls, frame_doc_cls = _create_dataset(\n\u001b[32m    324\u001b[39m                 self, name, persistent=persistent, **kwargs\n\u001b[32m    325\u001b[39m             )\n\u001b[32m    326\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj, name, persistent, _patches, _frames, _clips, _src_collection)\u001b[39m\n\u001b[32m   9280\u001b[39m     _frames=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   9281\u001b[39m     _clips=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   9282\u001b[39m     _src_collection=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   9283\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m9284\u001b[39m     slug = _validate_dataset_name(name)\n\u001b[32m   9285\u001b[39m \n\u001b[32m   9286\u001b[39m     _id = ObjectId()\n\u001b[32m   9287\u001b[39m     now = datetime.utcnow()\n",
      "\u001b[32mC:\\Machine Learning\\projects\\HackEurope_2026\\processing_layer\\.venv\\Lib\\site-packages\\fiftyone\\core\\dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(name, skip)\u001b[39m\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clashing_name_doc \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m         clashing_name = clashing_name_doc[\u001b[33m\"name\"\u001b[39m]\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m clashing_name == name:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Dataset name '{name}' is not available\")\n\u001b[32m    131\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    132\u001b[39m             raise ValueError(\n\u001b[32m    133\u001b[39m                 f\"Dataset name '{name}' is not available: slug '{slug}' \"\n",
      "\u001b[31mValueError\u001b[39m: Dataset name 'Voxel51/high-quality-invoice-images-for-ocr' is not available"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "N_SAMPLES = 1  # adjust as needed\n",
    "\n",
    "print(f\"Loading {N_SAMPLES} samples from HuggingFace…\")\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/high-quality-invoice-images-for-ocr\",\n",
    "    max_samples=N_SAMPLES,\n",
    ")\n",
    "\n",
    "annotated = [s for s in dataset if s[\"json_annotation\"] is not None]\n",
    "print(f\"{len(annotated)} annotated / {len(dataset)} loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_layer.extraction.invoice import InvoiceExtractor\n",
    "from processing_layer.llm.gemini import GeminiProvider\n",
    "\n",
    "provider = GeminiProvider()\n",
    "extractor = InvoiceExtractor(provider)\n",
    "print(f\"Using model: {provider.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(annotated):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Sample {i+1}: {Path(sample.filepath).name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # --- plot invoice image ---\n",
    "    img = Image.open(sample.filepath)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(6, 8))\n",
    "    axes.imshow(img)\n",
    "    axes.axis(\"off\")\n",
    "    axes.set_title(Path(sample.filepath).name, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- call Gemini ---\n",
    "    image_bytes = Path(sample.filepath).read_bytes()\n",
    "    extracted = extractor.extract_from_image(image_bytes, \"image/jpeg\")\n",
    "    gt = json.loads(sample[\"json_annotation\"])\n",
    "\n",
    "    results.append({\"sample\": Path(sample.filepath).name, \"extracted\": extracted, \"ground_truth\": gt})\n",
    "\n",
    "    # --- side-by-side comparison ---\n",
    "    print(\"\\n--- Extracted (Gemini) ---\")\n",
    "    print(extracted.model_dump_json(indent=2))\n",
    "\n",
    "    print(\"\\n--- Ground truth ---\")\n",
    "    print(json.dumps(gt, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Field-level summary ──────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    ext = r[\"extracted\"]\n",
    "    gt_inv = r[\"ground_truth\"][\"invoice\"]\n",
    "    gt_sub = r[\"ground_truth\"].get(\"subtotal\", {})\n",
    "    summary_rows.append({\n",
    "        \"file\": r[\"sample\"],\n",
    "        \"invoice_number\": ext.invoice_number == gt_inv.get(\"invoice_number\"),\n",
    "        \"invoice_date\":   ext.invoice_date   == gt_inv.get(\"invoice_date\"),\n",
    "        \"vendor_name\":    ext.vendor_name    == gt_inv.get(\"seller_name\"),\n",
    "        \"client_name\":    ext.client_name    == gt_inv.get(\"client_name\"),\n",
    "        \"vendor_address\": ext.vendor_address == gt_inv.get(\"seller_address\"),\n",
    "        \"client_address\": ext.client_address == gt_inv.get(\"client_address\"),\n",
    "        \"total_extracted\": ext.total,\n",
    "        \"total_gt\":        float(gt_sub.get(\"total\", \"nan\") or \"nan\"),\n",
    "        \"total_match\":     ext.total is not None and abs(ext.total - float(gt_sub.get(\"total\", 0) or 0)) < 0.01,\n",
    "        \"n_items\":         len(ext.line_items) == len(r[\"ground_truth\"].get(\"items\", [])),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Per-line-item price comparison ───────────────────────────────────────────\n",
    "item_rows = []\n",
    "for r in results:\n",
    "    ext_items = r[\"extracted\"].line_items\n",
    "    gt_items  = r[\"ground_truth\"].get(\"items\", [])\n",
    "    for idx, (ext_item, gt_item) in enumerate(zip(ext_items, gt_items)):\n",
    "        gt_total  = float(gt_item.get(\"total_price\", \"nan\") or \"nan\")\n",
    "        ext_total = ext_item.total_price\n",
    "        diff      = ext_total - gt_total if (ext_total is not None and not pd.isna(gt_total)) else None\n",
    "        pct_diff  = (diff / gt_total * 100) if (diff is not None and gt_total != 0) else None\n",
    "        item_rows.append({\n",
    "            \"file\":        r[\"sample\"],\n",
    "            \"item_idx\":    idx + 1,\n",
    "            \"description\": ext_item.description[:55] + \"…\" if len(ext_item.description) > 55 else ext_item.description,\n",
    "            \"qty\":         ext_item.quantity,\n",
    "            \"total_extracted\": ext_total,\n",
    "            \"total_gt\":        gt_total,\n",
    "            \"diff\":            round(diff, 2) if diff is not None else None,\n",
    "            \"pct_diff\":        round(pct_diff, 1) if pct_diff is not None else None,\n",
    "            \"exact_match\":     diff is not None and abs(diff) < 0.01,\n",
    "        })\n",
    "\n",
    "df_items = pd.DataFrame(item_rows)\n",
    "\n",
    "# highlight mismatches\n",
    "def color_match(val):\n",
    "    if val is True:  return \"background-color: #c8f7c5\"\n",
    "    if val is False: return \"background-color: #f7c5c5\"\n",
    "    return \"\"\n",
    "\n",
    "display(\n",
    "    df_items.style\n",
    "        .applymap(color_match, subset=[\"exact_match\"])\n",
    "        .format({\"pct_diff\": \"{:+.1f}%\", \"diff\": \"{:+.2f}\"}, na_rep=\"—\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
